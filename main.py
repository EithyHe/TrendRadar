#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TrendRadar - å¤šå¹³å°çƒ­ç‚¹èµ„è®¯ç›‘æ§åˆ†æç³»ç»Ÿï¼ˆå†å²åˆ†æå¢å¼ºç‰ˆï¼‰
ä½œè€…ï¼šåŸºäºåŸé¡¹ç›®æ”¹è¿›
åŠŸèƒ½ï¼šæ”¯æŒå†å²æ•°æ®æ±‡æ€»ã€è¶‹åŠ¿åˆ†æã€å¤šç»´åº¦ç»Ÿè®¡
"""

import requests
import json
import os
import re
import time
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import List, Dict, Any, Tuple
import html

# ====== é…ç½®å‚æ•° ======
CONFIG = {
    "FEISHU_WEBHOOK_URL": os.environ.get("FEISHU_WEBHOOK_URL"),
    "HISTORY_DAYS": int(os.environ.get("HISTORY_DAYS", "1")),  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œé»˜è®¤1å¤©
    "HISTORY_HOURS": int(os.environ.get("HISTORY_HOURS", "0")),  # å°æ—¶æ•°ï¼Œé»˜è®¤0
    "CUSTOM_START_DATE": os.environ.get("CUSTOM_START_DATE", ""),  # è‡ªå®šä¹‰å¼€å§‹æ—¥æœŸ YYYY-MM-DD
    "CUSTOM_END_DATE": os.environ.get("CUSTOM_END_DATE", ""),  # è‡ªå®šä¹‰ç»“æŸæ—¥æœŸ YYYY-MM-DD
    "MAX_RESULTS_PER_PLATFORM": 50,  # å¢åŠ è·å–æ•°é‡ä»¥æ”¯æŒå†å²åˆ†æ
    "TOP_KEYWORDS_LIMIT": 20,  # å¢åŠ æ˜¾ç¤ºçš„å…³é”®è¯æ•°é‡
    "TOP_NEWS_PER_KEYWORD": 8,  # å¢åŠ æ¯ä¸ªå…³é”®è¯æ˜¾ç¤ºçš„æ–°é—»æ•°é‡
    "FEISHU_SHOW_VERSION_UPDATE": True,
    "CONTINUE_WITHOUT_FEISHU": True,  # æ²¡æœ‰é£ä¹¦é…ç½®ä¹Ÿç»§ç»­è¿è¡Œ
    "API_BASE_URL": "https://newsnow.busiyi.world/api/news",  # ä½¿ç”¨å¤‡ç”¨APIåœ°å€
    "BACKUP_API_URLS": [  # å¤‡ç”¨APIåˆ—è¡¨
        "https://api.newsnow.cc/api/news",
        "https://newsnow.busiyi.world/api/news", 
        "https://newsnow.cc/api/news"
    ],
    "REQUEST_DELAY": 0.3,  # å‡å°‘è¯·æ±‚é—´éš”ä»¥æé«˜æ•ˆç‡
    "ENABLE_TREND_ANALYSIS": True,  # å¯ç”¨è¶‹åŠ¿åˆ†æ
}

# ====== æ”¯æŒçš„å¹³å°é…ç½® ======
PLATFORMS = [
    ("toutiao", "ä»Šæ—¥å¤´æ¡"),
    ("baidu", "ç™¾åº¦çƒ­æœ"), 
    ("weibo", "å¾®åš"),
    ("zhihu", "çŸ¥ä¹"),
    ("douyin", "æŠ–éŸ³"),
    ("bilibili-hot-search", "bilibiliçƒ­æœ"),
    ("wallstreetcn-hot", "åå°”è¡—è§é—»"),
    ("thepaper", "æ¾æ¹ƒæ–°é—»"),
    ("cls-hot", "è´¢è”ç¤¾çƒ­é—¨"),
    ("ifeng", "å‡¤å‡°ç½‘"),
    ("tieba", "è´´å§"),
]

class TrendAnalyzer:
    def __init__(self):
        self.frequency_words = self.load_frequency_words()
        self.filter_words = self.load_filter_words()
        self.must_words = self.load_must_words()
        self.all_data = []
        self.analysis_result = {}
        self.time_range = self.calculate_time_range()
        
    def calculate_time_range(self) -> Dict[str, Any]:
        """è®¡ç®—åˆ†æçš„æ—¶é—´èŒƒå›´"""
        now = datetime.now()
        
        # å¦‚æœè®¾ç½®äº†è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´
        if CONFIG['CUSTOM_START_DATE'] and CONFIG['CUSTOM_END_DATE']:
            try:
                start_date = datetime.strptime(CONFIG['CUSTOM_START_DATE'], '%Y-%m-%d')
                end_date = datetime.strptime(CONFIG['CUSTOM_END_DATE'], '%Y-%m-%d')
                
                # ç¡®ä¿ç»“æŸæ—¥æœŸä¸æ™šäºä»Šå¤©
                if end_date > now:
                    end_date = now
                
                # ç¡®ä¿å¼€å§‹æ—¥æœŸä¸æ™šäºç»“æŸæ—¥æœŸ
                if start_date > end_date:
                    start_date = end_date - timedelta(days=1)
                
                return {
                    'start_date': start_date,
                    'end_date': end_date,
                    'description': f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}",
                    'mode': 'custom_range'
                }
            except ValueError as e:
                print(f"âš ï¸ è‡ªå®šä¹‰æ—¥æœŸæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        
        # å¦‚æœåªè®¾ç½®äº†å¼€å§‹æ—¥æœŸ
        elif CONFIG['CUSTOM_START_DATE']:
            try:
                start_date = datetime.strptime(CONFIG['CUSTOM_START_DATE'], '%Y-%m-%d')
                if start_date > now:
                    start_date = now - timedelta(days=1)
                
                return {
                    'start_date': start_date,
                    'end_date': now,
                    'description': f"{start_date.strftime('%Y-%m-%d')} è‡³ä»Š",
                    'mode': 'from_date'
                }
            except ValueError as e:
                print(f"âš ï¸ å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        
        # ä½¿ç”¨å¤©æ•°+å°æ—¶æ•°è®¾ç½®
        total_hours = CONFIG['HISTORY_DAYS'] * 24 + CONFIG['HISTORY_HOURS']
        if total_hours <= 0:
            total_hours = 24  # é»˜è®¤24å°æ—¶
        
        start_time = now - timedelta(hours=total_hours)
        
        return {
            'start_date': start_time,
            'end_date': now,
            'description': f"è¿‡å» {CONFIG['HISTORY_DAYS']} å¤© {CONFIG['HISTORY_HOURS']} å°æ—¶",
            'mode': 'duration'
        }
        
    def load_frequency_words(self) -> List[str]:
        """åŠ è½½é¢‘ç‡è¯"""
        try:
            with open('frequency_words.txt', 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    return []
                
                words = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('!') and not line.startswith('+'):
                        words.append(line)
                return words
        except FileNotFoundError:
            print("âš ï¸ frequency_words.txt æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤å…³é”®è¯")
            return ["AI", "äººå·¥æ™ºèƒ½", "è‚¡å¸‚", "æˆ¿ä»·", "æ–°èƒ½æº", "æ•™è‚²", "å°±ä¸š"]
        except Exception as e:
            print(f"âš ï¸ è¯»å–frequency_words.txtå¤±è´¥: {e}")
            return []
    
    def load_filter_words(self) -> List[str]:
        """åŠ è½½è¿‡æ»¤è¯ï¼ˆä»¥!å¼€å¤´ï¼‰"""
        try:
            with open('frequency_words.txt', 'r', encoding='utf-8') as f:
                content = f.read().strip()
                words = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line.startswith('!'):
                        words.append(line[1:])  # å»æ‰!å·
                return words
        except:
            return []
    
    def load_must_words(self) -> List[str]:
        """åŠ è½½å¿…é¡»è¯ï¼ˆä»¥+å¼€å¤´ï¼‰"""
        try:
            with open('frequency_words.txt', 'r', encoding='utf-8') as f:
                content = f.read().strip()
                words = []
                for line in content.split('\n'):
                    line = line.strip()
                    if line.startswith('+'):
                        words.append(line[1:])  # å»æ‰+å·
                return words
        except:
            return []

    def fetch_platform_data(self, platform_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šå¹³å°çš„æ•°æ®"""
        try:
            url = f"{CONFIG['API_BASE_URL']}/{platform_id}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json',
                'Referer': 'https://newsnow.cc/'
            }
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            data = response.json()
            
            # æ ¹æ®ä¸åŒå¹³å°çš„æ•°æ®ç»“æ„è¿›è¡Œè§£æ
            if isinstance(data, dict):
                news_list = data.get('data', data.get('list', []))
            elif isinstance(data, list):
                news_list = data
            else:
                return []
            
            results = []
            for i, item in enumerate(news_list[:CONFIG['MAX_RESULTS_PER_PLATFORM']], 1):
                processed_item = {
                    'title': self.clean_text(str(item.get('title', item.get('name', ''))).strip()),
                    'url': item.get('url', item.get('link', '')),
                    'rank': i,
                    'platform_id': platform_id,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'raw_data': item
                }
                
                if processed_item['title']:
                    results.append(processed_item)
            
            return results
            
        except requests.exceptions.Timeout:
            print(f"  â° {platform_id} è¯·æ±‚è¶…æ—¶")
            return []
        except requests.exceptions.RequestException as e:
            print(f"  âŒ {platform_id} ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return []
        except json.JSONDecodeError as e:
            print(f"  âŒ {platform_id} æ•°æ®è§£æå¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"  âŒ {platform_id} æœªçŸ¥é”™è¯¯: {e}")
            return []

    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # HTMLè§£ç 
        text = html.unescape(text)
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = re.sub(r'[^\u4e00-\u9fff\w\s\-\.\!\?\,\:\;\(\)\[\]\"\'\/]', '', text)
        
        return text

    def collect_all_data(self) -> List[Dict]:
        """æ”¶é›†æ‰€æœ‰å¹³å°æ•°æ®"""
        print(f"ğŸ“Š å¼€å§‹æ”¶é›† {len(PLATFORMS)} ä¸ªå¹³å°çš„æ•°æ®...")
        
        all_data = []
        success_count = 0
        
        for platform_id, platform_name in PLATFORMS:
            try:
                print(f"  æ­£åœ¨è·å– {platform_name} æ•°æ®...")
                data = self.fetch_platform_data(platform_id)
                
                if data:
                    # ä¸ºæ¯æ¡æ•°æ®æ·»åŠ å¹³å°ä¿¡æ¯
                    for item in data:
                        item['platform_name'] = platform_name
                    all_data.extend(data)
                    success_count += 1
                    print(f"    âœ… è·å–åˆ° {len(data)} æ¡æ•°æ®")
                else:
                    print(f"    âš ï¸ æœªè·å–åˆ°æ•°æ®")
                
                # è¯·æ±‚é—´éš”
                time.sleep(CONFIG['REQUEST_DELAY'])
                
            except Exception as e:
                print(f"    âŒ {platform_name} æ•°æ®è·å–å¤±è´¥: {e}")
        
        print(f"ğŸ“ˆ æ•°æ®æ”¶é›†å®Œæˆï¼š{success_count}/{len(PLATFORMS)} ä¸ªå¹³å°æˆåŠŸï¼Œå…± {len(all_data)} æ¡æ•°æ®")
        return all_data

    def match_keywords(self, title: str) -> Tuple[List[str], bool]:
        """åŒ¹é…å…³é”®è¯å¹¶æ£€æŸ¥è¿‡æ»¤æ¡ä»¶"""
        if not title:
            return [], False
        
        title_lower = title.lower()
        
        # æ£€æŸ¥è¿‡æ»¤è¯
        for filter_word in self.filter_words:
            if filter_word.lower() in title_lower:
                return [], True  # è¢«è¿‡æ»¤
        
        # æ£€æŸ¥å¿…é¡»è¯ï¼ˆæ‰€æœ‰å¿…é¡»è¯éƒ½å¿…é¡»åŒ…å«ï¼‰
        if self.must_words:
            must_match_count = 0
            for must_word in self.must_words:
                if must_word.lower() in title_lower:
                    must_match_count += 1
            
            if must_match_count < len(self.must_words):
                return [], False  # å¿…é¡»è¯ä¸å®Œæ•´
        
        # æ£€æŸ¥é¢‘ç‡è¯
        matched_words = []
        for word in self.frequency_words:
            if word.lower() in title_lower:
                matched_words.append(word)
        
        return matched_words, False

    def analyze_data(self) -> Dict[str, Any]:
        """åˆ†ææ•°æ®"""
        print("ğŸ” å¼€å§‹æ•°æ®åˆ†æ...")
        
        # åŸºç¡€ç»Ÿè®¡
        total_items = len(self.all_data)
        platform_stats = Counter(item['platform_name'] for item in self.all_data)
        
        # å…³é”®è¯åŒ¹é…ç»Ÿè®¡
        keyword_matches = defaultdict(list)
        filtered_count = 0
        matched_count = 0
        
        for item in self.all_data:
            title = item.get('title', '')
            matched_keywords, is_filtered = self.match_keywords(title)
            
            if is_filtered:
                filtered_count += 1
                continue
            
            if matched_keywords:
                matched_count += 1
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…³é”®è¯ä½œä¸ºä¸»è¦åˆ†ç±»
                primary_keyword = matched_keywords[0]
                keyword_matches[primary_keyword].append({
                    'title': title,
                    'platform': item['platform_name'],
                    'rank': item.get('rank', 999),
                    'url': item.get('url', ''),
                    'timestamp': item.get('timestamp', ''),
                    'all_matched_keywords': matched_keywords
                })
        
        # å…³é”®è¯çƒ­åº¦æ’åº
        keyword_popularity = {
            keyword: len(items) 
            for keyword, items in keyword_matches.items()
        }
        
        sorted_keywords = sorted(
            keyword_popularity.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        # çƒ­é—¨æ–°é—»æ’åºï¼ˆæŒ‰æ’åï¼‰
        for keyword in keyword_matches:
            keyword_matches[keyword].sort(key=lambda x: x['rank'])
        
        analysis_result = {
            'total_items': total_items,
            'matched_count': matched_count,
            'filtered_count': filtered_count,
            'platform_stats': dict(platform_stats),
            'keyword_matches': dict(keyword_matches),
            'keyword_popularity': keyword_popularity,
            'sorted_keywords': sorted_keywords,
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'config_summary': {
                'frequency_words_count': len(self.frequency_words),
                'filter_words_count': len(self.filter_words),
                'must_words_count': len(self.must_words),
                'platforms_count': len(PLATFORMS)
            }
        }
        
        print(f"âœ… åˆ†æå®Œæˆï¼šæ€»è®¡ {total_items} æ¡æ•°æ®ï¼ŒåŒ¹é… {matched_count} æ¡ï¼Œè¿‡æ»¤ {filtered_count} æ¡")
        return analysis_result

    def generate_feishu_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé£ä¹¦æ¨é€æŠ¥å‘Š"""
        result = self.analysis_result
        time_desc = self.time_range['description']
        
        if not result['keyword_matches']:
            return {
                'total_titles': f"0 {result['analysis_time']} å†å²æ±‡æ€»",
                'timestamp': f"ğŸ“Š åˆ†ææ—¶é—´ï¼š{result['analysis_time']}",
                'report_type': f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{time_desc}",
                'text': f"ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡\n\n{self.generate_no_match_suggestion()}"
            }
        
        # ç”Ÿæˆä¸»è¦å†…å®¹
        text_parts = []
        total_matched = result['matched_count']
        
        # æ¦‚è§ˆä¿¡æ¯
        text_parts.append(f"ğŸ“Š çƒ­ç‚¹æ•°æ®æ¦‚è§ˆ")
        text_parts.append(f"ğŸ“… åˆ†æå‘¨æœŸï¼š{time_desc}")
        text_parts.append(f"â€¢ æ€»æ•°æ®é‡ï¼š{result['total_items']} æ¡")
        text_parts.append(f"â€¢ åŒ¹é…çƒ­ç‚¹ï¼š{result['matched_count']} æ¡")
        text_parts.append(f"â€¢ å…³é”®è¯å‘½ä¸­ï¼š{len(result['keyword_matches'])} ä¸ª")
        text_parts.append("")
        
        # æ—¶é—´è·¨åº¦ä¿¡æ¯
        if CONFIG['ENABLE_TREND_ANALYSIS'] and result.get('time_distribution'):
            text_parts.append("â° æ—¶é—´åˆ†å¸ƒï¼š")
            for time_period, count in list(result['time_distribution'].items())[:5]:
                text_parts.append(f"â€¢ {time_period}ï¼š{count} æ¡")
            text_parts.append("")
        
        # å¹³å°åˆ†å¸ƒ
        text_parts.append("ğŸ“± å¹³å°åˆ†å¸ƒï¼š")
        sorted_platforms = sorted(result['platform_stats'].items(), key=lambda x: x[1], reverse=True)
        for platform, count in sorted_platforms[:8]:  # æ˜¾ç¤ºå‰8ä¸ªå¹³å°
            text_parts.append(f"â€¢ {platform}ï¼š{count} æ¡")
        text_parts.append("")
        
        # çƒ­é—¨å…³é”®è¯
        text_parts.append("ğŸ”¥ çƒ­é—¨å…³é”®è¯æ’è¡Œï¼š")
        for i, (keyword, count) in enumerate(result['sorted_keywords'][:CONFIG['TOP_KEYWORDS_LIMIT']], 1):
            # è®¡ç®—å…³é”®è¯çš„è¶‹åŠ¿ï¼ˆå¦‚æœå¯ç”¨äº†è¶‹åŠ¿åˆ†æï¼‰
            trend_indicator = ""
            if CONFIG['ENABLE_TREND_ANALYSIS'] and result.get('keyword_trends', {}).get(keyword):
                trend_data = result['keyword_trends'][keyword]
                if trend_data.get('is_rising'):
                    trend_indicator = " ğŸ“ˆ"
                elif trend_data.get('is_declining'):
                    trend_indicator = " ğŸ“‰"
                elif trend_data.get('is_stable'):
                    trend_indicator = " â¡ï¸"
            
            text_parts.append(f"{i}. **{keyword}** - {count} æ¡{trend_indicator}")
        text_parts.append("")
        
        # è¯¦ç»†çƒ­ç‚¹å†…å®¹
        text_parts.append("ğŸ“° çƒ­ç‚¹è¯¦æƒ…ï¼š")
        for keyword, count in result['sorted_keywords'][:5]:  # è¯¦ç»†æ˜¾ç¤ºå‰5ä¸ªå…³é”®è¯
            text_parts.append(f"\nğŸ” **{keyword}** ({count} æ¡)ï¼š")
            
            items = result['keyword_matches'][keyword][:CONFIG['TOP_NEWS_PER_KEYWORD']]
            for j, item in enumerate(items, 1):
                rank_indicator = f"[{item['rank']}]" if item['rank'] <= 5 else f"[{item['rank']}]"
                title_preview = item['title'][:60] + "..." if len(item['title']) > 60 else item['title']
                
                # æ·»åŠ æ—¶é—´ä¿¡æ¯
                time_info = ""
                if item.get('item_time'):
                    time_str = item['item_time'].strftime('%m-%d %H:%M')
                    time_info = f" - {time_str}"
                
                text_parts.append(f"  {j}. [{item['platform']}] {title_preview} {rank_indicator}{time_info}")
        
        # æ„å»ºè¿”å›ç»“æœ
        return {
            'total_titles': f"{total_matched} {result['analysis_time']} å†å²æ±‡æ€»",
            'timestamp': f"â° åˆ†ææ—¶é—´ï¼š{result['analysis_time']}",
            'report_type': f"ğŸ“… {time_desc} - {len(result['keyword_matches'])} ä¸ªå…³é”®è¯å‘½ä¸­",
            'text': '\n'.join(text_parts)
        }

    def generate_no_match_suggestion(self) -> str:
        """ç”Ÿæˆæ— åŒ¹é…æ—¶çš„å»ºè®®"""
        suggestions = [
            "ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼š",
            "",
            "1. **æ‰©å¤§å…³é”®è¯èŒƒå›´**",
            "   â€¢ æ·»åŠ æ›´é€šç”¨çš„è¯æ±‡ï¼ˆå¦‚ï¼šç§‘æŠ€ã€æ•™è‚²ã€å¥åº·ï¼‰",
            "   â€¢ å‡å°‘è¿‡äºå…·ä½“çš„ä¸“ä¸šæœ¯è¯­",
            "",
            "2. **æ£€æŸ¥è¿‡æ»¤è¯è®¾ç½®**",
            f"   â€¢ å½“å‰æœ‰ {len(self.filter_words)} ä¸ªè¿‡æ»¤è¯",
            "   â€¢ ç¡®è®¤æ˜¯å¦è¿‡åº¦è¿‡æ»¤",
            "",
            "3. **è°ƒæ•´å¿…é¡»è¯é€»è¾‘**",
            f"   â€¢ å½“å‰æœ‰ {len(self.must_words)} ä¸ªå¿…é¡»è¯",
            "   â€¢ å¿…é¡»è¯è¦æ±‚åŒæ—¶åŒ…å«ï¼Œå¯èƒ½è¿‡äºä¸¥æ ¼",
            "",
            "4. **å½“å‰é…ç½®çŠ¶æ€**",
            f"   â€¢ ç›‘æ§å…³é”®è¯ï¼š{len(self.frequency_words)} ä¸ª",
            f"   â€¢ æ•°æ®å¹³å°ï¼š{len(PLATFORMS)} ä¸ª",
            f"   â€¢ æ€»æ•°æ®é‡ï¼š{self.analysis_result.get('total_items', 0)} æ¡",
            "",
            "å»ºè®®å…ˆå°è¯•æ·»åŠ ä¸€äº›é€šç”¨çƒ­é—¨å…³é”®è¯ï¼"
        ]
        return '\n'.join(suggestions)

    def send_feishu_message(self, message_data: Dict[str, Any]) -> bool:
        """å‘é€é£ä¹¦æ¶ˆæ¯"""
        if not CONFIG['FEISHU_WEBHOOK_URL']:
            print("âš ï¸ æœªé…ç½®é£ä¹¦Webhook URLï¼Œè·³è¿‡æ¨é€")
            return False
        
        try:
            payload = {
                "msg_type": "text",
                "content": {
                    "text": f"{message_data['total_titles']}\n\n{message_data['timestamp']}\n{message_data['report_type']}\n\n{message_data['text']}"
                }
            }
            
            response = requests.post(
                CONFIG['FEISHU_WEBHOOK_URL'],
                json=payload,
                timeout=10
            )
            
            if response.status_code == 200:
                print("âœ… é£ä¹¦æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                print(f"âŒ é£ä¹¦æ¶ˆæ¯å‘é€å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ é£ä¹¦æ¶ˆæ¯å‘é€å¼‚å¸¸ï¼š{e}")
            return False

    def save_html_report(self) -> None:
        """ä¿å­˜HTMLæŠ¥å‘Š"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
            output_dir = f"output/{today}/html"
            os.makedirs(output_dir, exist_ok=True)
            
            html_content = self.generate_html_report()
            
            # ä¿å­˜å½“æ—¥æŠ¥å‘Š
            daily_file = f"{output_dir}/å½“æ—¥ç»Ÿè®¡.html"
            with open(daily_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # ä¿å­˜æ ¹ç›®å½•index.html
            with open('index.html', 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            print(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ä¿å­˜ï¼š{daily_file}")
            
        except Exception as e:
            print(f"âŒ HTMLæŠ¥å‘Šä¿å­˜å¤±è´¥ï¼š{e}")

    def generate_html_report(self) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        result = self.analysis_result
        
        html_template = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TrendRadar - çƒ­ç‚¹åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 0; padding: 20px; background: #f5f7fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); overflow: hidden; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; }}
        .header h1 {{ margin: 0; font-size: 2.5em; font-weight: 700; }}
        .header p {{ margin: 10px 0 0; opacity: 0.9; font-size: 1.1em; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; padding: 30px; background: #f8f9ff; }}
        .stat-card {{ background: white; padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 2px 10px rgba(0,0,0,0.05); }}
        .stat-number {{ font-size: 2.5em; font-weight: bold; color: #667eea; margin-bottom: 5px; }}
        .stat-label {{ color: #666; font-size: 0.9em; }}
        .content {{ padding: 30px; }}
        .section {{ margin-bottom: 40px; }}
        .section h2 {{ color: #333; border-bottom: 3px solid #667eea; padding-bottom: 10px; margin-bottom: 20px; }}
        .keyword-grid {{ display: grid; grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); gap: 20px; }}
        .keyword-card {{ border: 1px solid #e1e5e9; border-radius: 8px; padding: 20px; background: #fff; }}
        .keyword-title {{ font-size: 1.3em; font-weight: bold; color: #667eea; margin-bottom: 15px; }}
        .news-item {{ margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 5px; border-left: 4px solid #667eea; }}
        .news-item a {{ text-decoration: none; color: #333; }}
        .news-item a:hover {{ color: #667eea; }}
        .platform-tag {{ display: inline-block; background: #e3f2fd; color: #1976d2; padding: 2px 8px; border-radius: 12px; font-size: 0.8em; margin-right: 5px; }}
        .rank-high {{ background: #ffebee; color: #c62828; }}
        .rank-normal {{ background: #f3e5f5; color: #7b1fa2; }}
        .footer {{ text-align: center; padding: 20px; background: #f5f7fa; color: #666; }}
        .no-data {{ text-align: center; padding: 60px 20px; color: #666; }}
        .no-data h3 {{ color: #ff6b6b; margin-bottom: 20px; }}
        .suggestion {{ background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 20px; margin: 20px 0; }}
        .suggestion h4 {{ color: #d68910; margin-top: 0; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ TrendRadar</h1>
            <p>å¤šå¹³å°çƒ­ç‚¹èµ„è®¯ç›‘æ§åˆ†æç³»ç»Ÿ - å†å²æ•°æ®æ±‡æ€»</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{result.get('total_items', 0)}</div>
                <div class="stat-label">æ€»æ•°æ®é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{result.get('matched_count', 0)}</div>
                <div class="stat-label">åŒ¹é…çƒ­ç‚¹</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(result.get('keyword_matches', {}))}</div>
                <div class="stat-label">å…³é”®è¯å‘½ä¸­</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{len(PLATFORMS)}</div>
                <div class="stat-label">ç›‘æ§å¹³å°</div>
            </div>
        </div>
        
        <div class="content">
        """
        
        if result.get('keyword_matches'):
            html_template += f"""
            <div class="section">
                <h2>ğŸ”¥ çƒ­é—¨å…³é”®è¯æ’è¡Œ</h2>
                <div class="keyword-grid">
            """
            
            for keyword, count in result['sorted_keywords'][:12]:  # æ˜¾ç¤ºå‰12ä¸ª
                items = result['keyword_matches'][keyword][:5]  # æ¯ä¸ªå…³é”®è¯æ˜¾ç¤ºå‰5æ¡
                
                html_template += f"""
                <div class="keyword-card">
                    <div class="keyword-title">{keyword} ({count} æ¡)</div>
                """
                
                for item in items:
                    rank_class = "rank-high" if item['rank'] <= 5 else "rank-normal"
                    url = item.get('url', '#')
                    
                    html_template += f"""
                    <div class="news-item">
                        <span class="platform-tag {rank_class}">{item['platform']} #{item['rank']}</span>
                        <br>
                        <a href="{url}" target="_blank" rel="noopener">{item['title']}</a>
                    </div>
                    """
                
                html_template += "</div>"
            
            html_template += "</div></div>"
            
        else:
            # æ— æ•°æ®æ—¶çš„æ˜¾ç¤º
            suggestions = self.generate_no_match_suggestion()
            html_template += f"""
            <div class="no-data">
                <h3>ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹æ•°æ®</h3>
                <div class="suggestion">
                    <h4>ğŸ’¡ ä¼˜åŒ–å»ºè®®</h4>
                    <pre style="white-space: pre-line; text-align: left;">{suggestions}</pre>
                </div>
            </div>
            """
        
        html_template += f"""
        </div>
        
        <div class="footer">
            <p>ğŸ“Š åˆ†ææ—¶é—´ï¼š{result.get('analysis_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))} | 
            ğŸ’¾ é…ç½®ï¼š{result.get('config_summary', {}).get('frequency_words_count', 0)} ä¸ªå…³é”®è¯ï¼Œ
            {result.get('config_summary', {}).get('platforms_count', 0)} ä¸ªå¹³å°</p>
            <p>âš¡ Powered by TrendRadar - è®©çƒ­ç‚¹è§¦æ‰‹å¯åŠ</p>
        </div>
    </div>
</body>
</html>
        """
        
        return html_template

    def run(self) -> None:
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("ğŸš€ TrendRadar å†å²åˆ†æç‰ˆå¯åŠ¨...")
        print(f"ğŸ“‹ é…ç½®æ¦‚è§ˆï¼š{len(self.frequency_words)} ä¸ªå…³é”®è¯ï¼Œ{len(PLATFORMS)} ä¸ªå¹³å°")
        print(f"â° åˆ†ææ—¶é—´èŒƒå›´ï¼š{self.time_range['description']}")
        
        # æ˜¾ç¤ºæ—¶é—´è®¾ç½®å¸®åŠ©ä¿¡æ¯
        self.print_time_config_help()
        
        try:
            # 1. æ”¶é›†æ•°æ®
            self.all_data = self.collect_all_data()
            
            if not self.all_data:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return
            
            # 2. åˆ†ææ•°æ®
            self.analysis_result = self.analyze_data()
            
            # 3. ç”Ÿæˆå¹¶å‘é€é£ä¹¦æŠ¥å‘Š
            if CONFIG['FEISHU_WEBHOOK_URL']:
                feishu_report = self.generate_feishu_report()
                self.send_feishu_message(feishu_report)
            elif not CONFIG['CONTINUE_WITHOUT_FEISHU']:
                print("âš ï¸ æœªé…ç½®é£ä¹¦æ¨é€ä¸”è®¾ç½®ä¸ºä¸ç»§ç»­è¿è¡Œ")
                return
            
            # 4. ä¿å­˜HTMLæŠ¥å‘Š
            self.save_html_report()
            
            # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_summary()
            
            print("âœ… TrendRadar è¿è¡Œå®Œæˆï¼")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥ï¼š{e}")
            import traceback
            traceback.print_exc()

    def print_time_config_help(self) -> None:
        """æ‰“å°æ—¶é—´é…ç½®å¸®åŠ©ä¿¡æ¯"""
        print("\n" + "="*60)
        print("â° æ—¶é—´èŒƒå›´é…ç½®è¯´æ˜")
        print("="*60)
        print("å½“å‰ä½¿ç”¨æ¨¡å¼ï¼š", self.time_range['mode'])
        print("å½“å‰æ—¶é—´èŒƒå›´ï¼š", self.time_range['description'])
        print()
        print("ğŸ’¡ å¦‚éœ€ä¿®æ”¹æ—¶é—´è®¾ç½®ï¼Œå¯åœ¨GitHub Secretsä¸­æ·»åŠ ï¼š")
        print("   â€¢ HISTORY_DAYS=7        # åˆ†æè¿‡å»7å¤©")
        print("   â€¢ HISTORY_HOURS=12      # é¢å¤–å¢åŠ 12å°æ—¶")
        print("   â€¢ CUSTOM_START_DATE=2024-01-01  # è‡ªå®šä¹‰å¼€å§‹æ—¥æœŸ")
        print("   â€¢ CUSTOM_END_DATE=2024-01-31    # è‡ªå®šä¹‰ç»“æŸæ—¥æœŸ")
        print()
        print("ğŸ“ ç¤ºä¾‹é…ç½®ï¼š")
        print("   1. åˆ†æè¿‡å»3å¤©ï¼šHISTORY_DAYS=3")
        print("   2. åˆ†æè¿‡å»1å‘¨ï¼šHISTORY_DAYS=7")
        print("   3. åˆ†æç‰¹å®šæœˆä»½ï¼šCUSTOM_START_DATE=2024-01-01, CUSTOM_END_DATE=2024-01-31")
        print("   4. åˆ†ææœ€è¿‘36å°æ—¶ï¼šHISTORY_DAYS=1, HISTORY_HOURS=12")
        print("="*60)
        
        try:
            # 1. æ”¶é›†æ•°æ®
            self.all_data = self.collect_all_data()
            
            if not self.all_data:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return
            
            # 2. åˆ†ææ•°æ®
            self.analysis_result = self.analyze_data()
            
            # 3. ç”Ÿæˆå¹¶å‘é€é£ä¹¦æŠ¥å‘Š
            if CONFIG['FEISHU_WEBHOOK_URL']:
                feishu_report = self.generate_feishu_report()
                self.send_feishu_message(feishu_report)
            elif not CONFIG['CONTINUE_WITHOUT_FEISHU']:
                print("âš ï¸ æœªé…ç½®é£ä¹¦æ¨é€ä¸”è®¾ç½®ä¸ºä¸ç»§ç»­è¿è¡Œ")
                return
            
            # 4. ä¿å­˜HTMLæŠ¥å‘Š
            self.save_html_report()
            
            # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_summary()
            
            print("âœ… TrendRadar è¿è¡Œå®Œæˆï¼")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        except Exception as e:
            print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥ï¼š{e}")
            import traceback
            traceback.print_exc()

    def print_summary(self) -> None:
        """æ‰“å°è¿è¡Œæ€»ç»“"""
        result = self.analysis_result
        print("\n" + "="*50)
        print("ğŸ“Š è¿è¡Œæ€»ç»“")
        print("="*50)
        print(f"ğŸ“ˆ æ•°æ®æ”¶é›†ï¼š{result['total_items']} æ¡")
        print(f"ğŸ¯ å…³é”®è¯åŒ¹é…ï¼š{result['matched_count']} æ¡")
        print(f"ğŸš« è¿‡æ»¤æ•°æ®ï¼š{result['filtered_count']} æ¡")
        print(f"ğŸ”¥ çƒ­é—¨å…³é”®è¯ï¼š{len(result['keyword_matches'])} ä¸ª")
        
        if result['sorted_keywords']:
            print(f"ğŸ† æœ€çƒ­å…³é”®è¯ï¼š{result['sorted_keywords'][0][0]} ({result['sorted_keywords'][0][1]} æ¡)")
        
        print(f"â° åˆ†ææ—¶é—´ï¼š{result['analysis_time']}")
        print("="*50)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = TrendAnalyzer()
    analyzer.run()


if __name__ == "__main__":
    main()
